## 前言
先前我们了解到神经网络的本质就是一个复杂的非线性函数，我们的目的就是为了计算其中的w和b使得函数去尽可能拟合真实的数据。

## 进入正题
我们依旧从最简单的线性函数说起
$$y=wx+b$$
什么样的w和b才算是好的呢？->能够让y尽可能接近真实数据,也就是拟合的好。
那如何评价拟合的好不好？->**损失函数**它来了。
### 损失函数
我们用$$y_{i}$$来表示真实数据
用$$\hat y_{i}$$表示预测数据

那么**损失函数的计算公式**其实就是$$\sum_{i=1}^{N}|y_{i} -\hat{y_{i}}|$$
但绝对值相对而言比较麻烦，直接采取**平方**会更加省事，也能放大预测数据和真实数据的差距，因此我们得到$$\sum_{i=1}^{N}(y_{i} -\hat{y_{i}})^{2}$$
然后我们再取个平均值来消除样本数量的影响:$$\frac{1}{N}\sum_{i=1}^{N}(y_{i} -\hat{y_{i}})^{2}$$
最终我们得到的这个公式就叫做**均方误差(MSE)**
而均方误差就是用来表示损失函数的一种，我们通常将损失（loss）函数记作$$L$$
那么从参数的视角来看，其实就是一个关于w和b的函数：$$L(w,b)=\frac{1}{N}\sum_{i=1}^{N}(y_{i} -\hat{y_{i}})^{2}$$
### 然后我们回顾我们的目的
**数据**：$$(x_{1},y_{1}),(x_{2},y_{2}),.....,(x_{n},y_{n})$$
**模型**：$$y=wx+b$$
**损失函数**：$$L(w,b)=\frac{1}{N}\sum_{i=1}^{N}(y_{i} -\hat{y_{i}})^{2}$$
**目标**：求解让L最小的w和b的值。
那怎么求呢？这其实就是我们所熟知的多元函数求最小值的问题——>**那就是对每个参数求偏导然后让其等于零。**
这个用线性函数模型来拟合x和y的过程就是机器学习中所说的**线性回归**
### 回到神经网络
正如前言所说，神经网络其实就是一个通过线性函数和非线性激活函数不断组合形成的一个超复杂的非线性函数，那么它对应的损失函数只会是更加复杂非线性函数，**求导也难以计算**，那怎么办？别忘了我们用的是计算机啊，那就......

**一点点试呗**

我们来点具体数值看看怎么个**一点点试**法，这里我们假设：$$w=5,b=5,L=10$$
接下来我们试着调整w，我们将w调大一点点：$$w=6,b=5,L=9$$
发现L变小了！说明w的调整方向对了，我们再来调整b：$$w=6,b=6,L=11$$
这说明b的增大会使得损失函数L增大，那我们就反过来把b变小：$$w=6,b=4,L=7$$
然后我们就这样循环往复的调整w和b：
$$w=8,b=4,L=5$$
$$w=8,b=3,L=3$$
$$w=9,b=3,L=1$$
$$......$$
就这样每次调整一点点，直到损失函数L足够小，那具体怎么调？回到一开始$$w=5,b=5,L=10$$
我们将w变化一点点，会使得损失函数变化了多少：$$w=6,b=5,L=9$$
**这变化了多少**这一部分其实就是损失函数对w的偏导数$$\frac{\partial L(w,b)}{\partial w}$$
对b来说同样如此：
$$\frac{\partial L(w,b)}{\partial b}$$
我们要做的其实就是让w和b不断的往偏导数的反方向去变化：
$$w=w-\frac{\partial L(w,b)}{\partial w}$$
$$b=b-\frac{\partial L(w,b)}{\partial b}$$
那具体变化的快慢呢，我们引入一个系数**η**来控制，这个就叫做**学习率**
而这些偏导数$$\eta \frac{\partial L(w,b)}{\partial w}，\eta \frac{\partial L(w,b)}{\partial b}$$所构成的向量就叫做**梯度**，而我们不断变化w和b使得损失函数L逐渐减小进而求出最终的w和b的这个过程就叫做**梯度下降**
那么问题又来了：$$\eta \frac{\partial L(w,b)}{\partial w}，\eta \frac{\partial L(w,b)}{\partial b}$$这堆偏导怎么求啊？
我们用一个简单的神经网络作为例子：
![链式法则](/my-learning-site/images/链式法则.png)
这一个过程其实就是：
$$x->g(w_{1}x+b_{1})->a->g(w_{2}a+b_{2})->\hat{y}->\frac{1}{N}\sum_{i=1}^{N}(y_{i} -\hat{y_{i}})^{2}->L$$
由于这里神经元只有一个，也就是N=1，我们稍微简化一点：
$$x->g(w_{1}x+b_{1})->a->g(w_{2}a+b_{2})->\hat{y}->(y_{i} -\hat{y_{i}})^{2}->L$$
我们直接拿最难的w1的偏导数来说：$$\frac{\partial L}{\partial w_{1}}$$
从偏导数要表达的思想来看，我们想看的其实就是w1变化一点点会使得L变化多少，那么同理，我们可以先看w1变化一点点会使得a变化多少，再看看a变化一点点会使得$\hat{y}$变化多少，再看$\hat{y}$变化一点点，会使得L变化多少。
$$\frac{\partial a}{\partial w_{1}},\frac{\partial\hat{y}}{\partial a},\frac{\partial L}{\partial \hat{y}}$$
我们把这3个玩意$×$起来，就会知道$L$变化多少了:
$$\frac{\partial L}{\partial w_{1}}=\frac{\partial a}{\partial w_{1}}\frac{\partial\hat{y}}{\partial a}\frac{\partial L}{\partial \hat{y}}$$
这就像一个齿轮组一样，第一个齿轮转一圈会使得最后一个齿轮转多少圈一样，这样的偏导数计算方法就叫做**链式法则**，其实就是数学中复合函数求导嘛，而这个过程，我们回到图形：![链式法则](/my-learning-site/images/链式法则.png)
可以发现我们其实是先计算的$\frac{\partial L}{\partial \hat{y}}$，也就是说我们是从后面输出层开始，从右向左以此求导，然后逐步更新每一层的参数，直到把所有的神经网络的参数都更新一遍，我们其实很清晰的看到，这些偏导数就这样从右到左依次 **“传播”** 下去，这个过程其实就是我们所说的 **“反向传播”**

根据前面所了解的知识，我们 **前向传播** 根据输入的x计算出y，然后再通过 **反向传播** 计算出损失函数关于每个参数的梯度，然后每个参数都向着梯度的反方向变化一点点，这就构成了神经网络的一次 **训练**，而神经网络经过多轮这样的 **训练**，里面的参数都一点一点的变化，直到损失函数足够小，也就是找到了我们想要的那个函数。

至此我们了解了不少基本概念了呢，但在面对实际问题的时候往往存在各种各样的问题，会遇到什么样的难题又该采取什么样的方法去解决呢？这就是之后的事情啦。