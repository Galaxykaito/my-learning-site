# 什么是向量化（Embedding）

## 从向量的本质说起

要理解什么是向量化，首先要明白**向量的本质**。

在数学中，我们常用一个点来表示位置，例如数轴上的 `0, 1, 2, 3, 4……` 每个数字都对应唯一的位置。如果我们用 `0, 2, 4, 6` 表示女生，用 `1, 3, 5, 7` 表示男生，那么 "双数" 和 "单数" 这一特征就成为定义性别的**维度**。

## 从一维到二维

在二维平面上，我们用 `(x, y)` 记录点的位置。若把点 `(x, y)` 与原点相连并画出指向该点的箭头，就得到了一个**向量**。

如果我们为 x、y 赋予具体含义，比如：
- **x** 表示水果的水分
- **y** 表示甜度

那么水分和甜度相似的水果，它们的向量在二维平面上就会靠得很近。

> 这种 "向量越接近，代表事物越相似" 的性质，正是向量可区分、可解释的核心体现。

## 高维向量的必要性

但现实中，一个苹果的特征远不止水分和甜度，还包括：
- 颜色
- 形状
- 口感
- 酸度
- 硬度
- ……

为了让机器理解文本的语义，我们需要将文字转化为向量，**这个过程才是向量化**。

## Transformer 中的向量化

在 Transformer 模型中，输入的文本会被直接编码为 **768 维的向量**。

这些向量的每个维度没有 "水分、甜度" 这样的直观含义，而是模型学习到的**抽象语义特征**。

## 降维：为了可视化和计算效率

由于 768 维向量不便于可视化和计算，我们会用降维算法（如 **PCA**、**t-SNE**）将其映射到二维或三维空间。

⚠️ **注意**：降维只是高维向量的后续处理步骤，**不是向量化本身**。

降维的目的：
1. 存储更高效
2. 计算更快捷
3. **实现高维数据的可视化**，让我们能直观地观察语义相似性
