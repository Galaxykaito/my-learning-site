# Transformer 怎么做到向量化的？

## Transformer 是什么？

**Transformer** 是一种深度学习模型架构，由 Google 在 2017 年提出（论文《Attention is All You Need》）。它最初用于机器翻译，后来成为了自然语言处理（NLP）领域的基础架构。

### 核心特点

- **自注意力机制（Self-Attention）** - 可以捕捉序列中任意两个位置之间的关系
- **并行计算** - 不像 RNN 需要逐步处理，可以同时处理整个序列
- **位置编码（Positional Encoding）** - 为每个词添加位置信息

目前几乎所有主流大语言模型（GPT、BERT、LLaMA 等）都基于 Transformer 架构。

---

## Transformer 如何将文本向量化？

### 第 1 步：分词（Tokenization）

将输入文本切分成更小的单位（token），例如：

```
"你好世界" → ["你", "好", "世", "界"]
```

或者用子词分割（BPE、WordPiece）：

```
"understanding" → ["under", "standing"]
```

### 第 2 步：词嵌入（Token Embedding）

每个 token 会被映射到一个固定维度的向量（如 768 维）。这个映射由一个**嵌入矩阵**完成，类似于查字典：

```
"你" → [0.23, -0.45, 0.78, ..., 0.12]  (768维)
"好" → [0.56, 0.12, -0.34, ..., 0.89]  (768维)
```

### 第 3 步：位置编码（Positional Encoding）

由于 Transformer 是并行处理的，它不知道词的顺序。因此需要添加位置信息：

```
最终向量 = 词嵌入向量 + 位置编码向量
```

位置编码使用正弦和余弦函数生成，确保每个位置都有唯一的编码。

### 第 4 步：自注意力机制（Self-Attention）

这是 Transformer 的核心！它让模型理解上下文关系。

例如在句子 "银行账户" 和 "河岸边" 中，"银行" 和 "河岸" 的词向量会根据上下文动态调整，产生不同的语义表示。

### 第 5 步：多层堆叠

经过多层 Transformer 编码器（通常 12 层或 24 层），最终输出每个 token 的**上下文感知向量**。

这些向量已经包含了：
- 词本身的语义
- 词在句子中的位置
- 词与其他词的关系

---

## 图片如何向量化？

图片的向量化与文本不同，但核心思想相似：**将像素信息转换为高维向量**。

### 方法 1：卷积神经网络（CNN）

传统的图像处理方法，广泛用于图像分类、目标检测等任务。

#### 工作原理

1. **卷积层** - 提取局部特征（边缘、纹理、形状）
2. **池化层** - 降低分辨率，保留关键信息
3. **全连接层** - 将二维特征图展平为一维向量

```
输入图片 (224×224×3)
  ↓ 卷积+池化
特征图 (7×7×512)
  ↓ 展平
向量 (2048维)
```

**例如**：ResNet-50 最后一层会输出 **2048 维向量**，这就是图片的向量化表示。

---

### 方法 2：Vision Transformer (ViT)

将 Transformer 应用于图像处理的方法，2020 年由 Google 提出。

#### 工作原理

1. **分块（Patch Embedding）**
   - 将图片切分成固定大小的小块（如 16×16 像素）
   - 一张 224×224 的图片会被切成 196 个小块（14×14）

2. **线性投影**
   - 每个小块展平为一维向量（16×16×3 = 768 维）
   - 通过线性层映射到统一的嵌入维度

3. **位置编码**
   - 为每个图块添加位置信息（类似文本的位置编码）

4. **Transformer 编码**
   - 将所有图块向量输入 Transformer
   - 通过自注意力机制学习图块之间的关系

5. **输出向量**
   - 最终输出 **[CLS] token** 的向量作为整张图片的表示

```
图片 (224×224×3)
  ↓ 切分成 14×14 个小块
196个图块 (每个16×16×3)
  ↓ 展平+线性投影
196个向量 (每个768维)
  ↓ Transformer编码器
最终向量 (768维)
```

---

### 方法 3：CLIP（对比学习）

由 OpenAI 提出，同时处理**图像和文本**，学习它们在同一向量空间中的对应关系。

#### 核心思想

- 图像用 **Vision Transformer** 或 **ResNet** 编码为向量
- 文本用 **Text Transformer** 编码为向量
- 训练目标：让匹配的图文对向量尽可能接近，不匹配的尽可能远离

#### 应用场景

- **图像搜索** - 输入文本 "一只猫在睡觉"，找到相似图片
- **零样本分类** - 不需要训练数据，直接用文本描述分类
- **文生图** - DALL-E、Stable Diffusion 等模型的基础

---

## 总结

| **数据类型** | **向量化方法** | **典型维度** | **代表模型** |
|------------|-------------|-----------|------------|
| 文本 | Token Embedding + Transformer | 768 / 1024 | BERT, GPT |
| 图片（CNN） | 卷积 + 池化 + 全连接 | 2048 | ResNet, VGG |
| 图片（ViT） | 图块切分 + Transformer | 768 | Vision Transformer |
| 图文（多模态） | 图像编码器 + 文本编码器 | 512 / 768 | CLIP |

无论是文本还是图片，向量化的核心目标都是：**将不同类型的数据映射到同一个高维向量空间，使得相似的内容在空间中靠得更近**。
