# 什么是神经网络？

## 让我们从函数开始理解

理解神经网络，我们需要从「函数」开始出发。

函数的基本形式是 $f(x) = y$，其中：
- $x$ 是输入
- $f(x)$ 是规则
- $y$ 是结果

在有着一些简单规律的情况下，$f(x) = y$ 都能得出一个精确的结果。而那些没那么容易发现的规则呢，我们也会试图用 $f(x) = y$ 来解释。也就是 $y = wx + b$，通过找到一组 $w$ 和 $b$ 来吻合规则，但通常又很难找到完全吻合的函数，于是便调整 $w$ 和 $b$ 使得其「拟合」「近似」。

所以，在早期的机器学习中，**「学习」就是不断找一组 $w$ 和 $b$ 使其不断拟合你的数据集**。

## 从线性到非线性

但数据千变万化，规则也不会一直是 $y = wx + b$ 的直线。这时候就需要让直线「弯」几下，在这样需求指导下，关于函数的研究也从**线性函数**进化到了**非线性函数**。

那怎么让 $y = wx + b$ 「弯」下来？

→ **套一个非线性运算就可以了**。

比如：
- $\sin(wx + b)$
- $\cos(wx + b)$
- $(wx + b)^2$
- $e^{wx + b}$
- ……

而这个套在 $wx + b$ 上的「非线性运算」就是我们所说的**「激活函数」**。函数也变成了 $f(x) = g(wx + b)$ 的形式。

### 常用的激活函数

- **Sigmoid 函数**：$\sigma(z) = \frac{1}{1 + e^{-z}}$
- **ReLU 函数**：$\text{ReLU}(z) = \max(0, z)$

## 多输入与多层网络

回到 $f(x) = g(wx + b)$。

实际上，$x$ 作为输入往往不会只有一个，即 $f(x) = g(w_{1}x_{1} + w_{2}x_{2} + b)$ 之类的情况，**每个输入都对应一个 $w$**。

而有时只套一层激活函数，函数 $f(x)$ 的曲线仍不够灵活，那怎么办？

那就在 $g(w_{1}x_{1} + w_{2}x_{2} + b)$ 的基础上进行一次线性变换，然后再套一层激活函数：

$$g(w_{3} \cdot g(w_{1}x_{1} + w_{2}x_{2} + b) + b_{2})$$

就这样无限套下去，理论上可以逼近任意的连续函数。但这样写下去很长，要换种表达方式。

## 神经网络的图形化表示

回到最开始的 $f(x) = g(wx + b)$，我们可以用图形化的方式来表示神经网络的结构：

![神经网络结构示意图](/my-learning-site/images/神经网络简图.png)

从图中可以看到：

- **输入层**（最左边）：接收原始数据 $x_{1}, x_{2}, ...$
- **隐藏层**（中间）：对输入进行线性变换 $wx + b$ 并应用激活函数 $g$
- **输出层**（最右边）：产生最终结果 $y$

每个圆圈代表一个**神经元**，负责对输入进行加权求和、加偏置、再通过激活函数处理。
我们再以$$g(w_{3} \cdot g(w_{1}x_{1} + w_{2}x_{2} + b) + b_{2})$$为例
输入层：$$（x_{1},x_{2}）$$
隐藏层(对输入做线性变换和一层激活函数）：$$g(w_{1}x_{1}+w_{2}x_{2}+b)$$
输出层（将前一层结果当作整体做线性变化和一层激活函数）：$$g(w_{3} \cdot g(w_{1}x_{1} + w_{2}x_{2} + b) + b_{2})$$
我们再回到这个简图中可以看到
![神经网络结构简图](/my-learning-site/images/神经网络简图.png)
从左到右表达式越来越复杂，就像是一个信号从左到右传递过去，这个过程就叫做神经网络的**前向传播**

而神经网络的每一层的神经元都可以无限叠加，隐藏层的层数也可以无限多，进而就可以构造出非常复杂非线性函数。虽然这个函数可能十分复杂，但我们的目标很明确，根据已知的x和y，去**猜**整个过程中的$$w$$和$$b$$是什么,而神经网络是怎么做的呢，请看文章《神经网络如何计算参数》