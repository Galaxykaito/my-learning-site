# 什么是神经网络？

## 从函数开始理解

理解神经网络，我们需要从「函数」开始出发。

函数的基本形式是 $f(x) = y$，其中：
- $x$ 是输入
- $f(x)$ 是规则
- $y$ 是结果

在有着一些简单规律的情况下，$f(x) = y$ 都能得出一个精确的结果。而那些没那么容易发现的规则呢，我们也会试图用 $f(x) = y$ 来解释。也就是 $y = wx + b$，通过找到一组 $w$ 和 $b$ 来吻合规则，但通常又很难找到完全吻合的函数，于是便调整 $w$ 和 $b$ 使得其「拟合」「近似」。

所以，在早期的机器学习中，**「学习」就是不断找一组 $w$ 和 $b$ 使其不断拟合你的数据集**。

## 从线性到非线性

但数据千变万化，规则也不会一直是 $y = wx + b$ 的直线。这时候就需要让直线「弯」几下，在这样需求指导下，关于函数的研究也从**线性函数**进化到了**非线性函数**。

那怎么让 $y = wx + b$ 「弯」下来？

→ **套一个非线性运算就可以了**。

比如：
- $\sin(wx + b)$
- $\cos(wx + b)$
- $(wx + b)^2$
- $e^{wx + b}$
- ……

而这个套在 $wx + b$ 上的「非线性运算」就是我们所说的**「激活函数」**。函数也变成了 $f(x) = g(wx + b)$ 的形式。

### 常用的激活函数

- **Sigmoid 函数**：$\sigma(z) = \frac{1}{1 + e^{-z}}$
- **ReLU 函数**：$\text{ReLU}(z) = \max(0, z)$

## 多输入与多层网络

回到 $f(x) = g(wx + b)$。

实际上，$x$ 作为输入往往不会只有一个，即 $f(x) = g(w_{1}x_{1} + w_{2}x_{2} + b)$ 之类的情况，**每个输入都对应一个 $w$**。

而有时只套一层激活函数，函数 $f(x)$ 的曲线仍不够灵活，那怎么办？

那就在 $g(w_{1}x_{1} + w_{2}x_{2} + b)$ 的基础上进行一次线性变换，然后再套一层激活函数：

$$g(w_{2} \cdot g(w_{1}x_{1} + w_{2}x_{2} + b) + b_{2})$$

就这样无限套下去，理论上可以逼近任意的连续函数。但这样写下去很长，要换种表达方式。

## 神经网络的图形化表示

回到最开始的 $f(x) = g(wx + b)$，可以表示为：

$$x \xrightarrow{w,b} \boxed{g} \to y$$

这里的每个方框就代表一个**「神经元」**（对输入进行线性变换和激活函数处理）。

### 多输入情况

当有多个输入时：

$$\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} \xrightarrow{w,b} \boxed{g} \to y$$

### 多层网络

当多套一层激活函数时：

$$\begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} \xrightarrow{w_{1},b_{1}} \boxed{g} \xrightarrow{w_{2},b_{2}} \boxed{g} \to y$$

这就是**神经网络**。

- 最左边：**输入层**
- 中间：**隐藏层**
- 最右边：**输出层**