# Transformer学习：词嵌入之Tokenizer

## 关于嵌入

![网络架构图](/my-learning-site/images/transformer网络架构.png)

嵌入是 Transformer 原理图的第一步（Input Embedding），本文就是要解决它是如何实现的？又为什么要这么做？

## 为什么需要嵌入？
首要原因就是「模型无法理解文字数据，只能理解数字，而仅数字本身并不包含语义」，**嵌入**的存在就是为了解决这个问题。

数字本身无意义，但嵌入通过学习为文字对应的向量赋予了「符合语义的数学关系」，让模型能通过数学运算理解文字的语义关联。

**核心逻辑**：语义关联 → 数学关系

嵌入的本质，就是完成一次语义到数学的映射——把人类能理解的「文字语义关联」，转化为模型能计算的「向量数学关系」，且这种转化是一一对应、符合直觉的：

| 人类视角 | 数学视角 |
|---------|----------|
| 语义相似的词（比如：学习/编程、苹果/香蕉） | 向量空间中距离更近（余弦相似度更高、欧氏距离更小） |
| 语义相关的词（比如：国王 - 男人 = 女王 - 女人） | 向量空间中满足线性运算规律 |
| 语义无关的词（比如：学习/苹果、编程/香蕉） | 向量空间中距离更远 |

**简单说**：嵌入没有创造意义，只是把人类语言的语义规律，翻译成了模型能懂的数学规律。

### 关键：不是「随便找数学关系」，而是「找和语义匹配的数学关系」

数字本身能产生无数种数学关系（比如 `7+8=15`，`9-7=2`），但这些纯数字的算术关系和语义无关（比如「我」=7、「爱」=8，`7+8` 的结果和「我爱」的语义毫无关系）。

而嵌入的核心价值在于：**通过海量语料的学习，筛选出和语义强绑定的数学关系，抛弃无意义的纯数字运算关系**。

比如：
- 模型从「学习编程」「学习代码」「练习编程」这些语料中，学到「学习」和「编程」的向量余弦相似度 ≈ 0.8（高相似）
- 从「吃苹果」「吃香蕉」「摘苹果」这些语料中，学到「苹果」和「香蕉」的向量余弦相似度 ≈ 0.75（高相似）
- 而「学习」和「苹果」的向量余弦相似度 ≈ 0.2（低相似）

这些相似度数值，就是和语义匹配的数学关系，模型通过计算这些关系，就能理解文字的语义。
## 嵌入都有哪些类型？

### 按技术分类
![嵌入类型](/my-learning-site/images/嵌入类型.png)

### 按任务场景分类（常见的）
![img.png](/my-learning-site/images/任务场景嵌入类型.png)

可以看到这些嵌入类型之间主要的区别就是**被用来嵌入的对象不同**，例如：
- 词嵌入的嵌入对象是**词**
- 句子嵌入的对象是**一整个句子**
- ……

那么我们弄明白词嵌入是什么，之后代换一下应该也基本上理解的也差不多了，开始我们的探究吧。

## 词嵌入是如何运作的？

从描述上看，词嵌入是「将单个单词表示为数字向量」。

例如，「我爱学习编程」就会被拆成「我」「爱」「学习」「编程」四个词，然后分别转换为向量。

那怎么转换成向量呢？

首先，机器不理解文字，它根本不认识「我」「爱」「学习」「编程」这些文字，更不用说理解它们之间的关系了。

那就像我们先前说的，让机器理解得先让它们变成机器能理解的数字。

如此，我们给这些文字分配一个初始的独特数字，也就是 **ID**，比如：
- 「我」= 0
- 「爱」= 1
- 「学习」= 2
- 「编程」= 3

这样我们就完成初步的「翻译」。

但是，这些零散的数字彼此之间并没有含义，机器又怎么知道「我爱学习编程」是一句完整的意思呢？这就引出了我们接下来要讲的 **Tokenizer**。

回到我们的目的，我们既然要给这些文字一个初始数字，那么它得先知道这一句话怎么拆成单词，这一步我们叫做 **Token 切分**。

### Tokenizer 的第一步：Token 切分

Token 切分其实是先于大模型存在的，在 Word2Vec、BERT 等大模型方法之前，NLP 任务中就已存在 Token 切分这一说法，是一种基于统计、规则的算法。

作为大模型训练的前置工作，Token 切分的目的是为了**把一个连续的自然语言拆成有语义、可复用的最小单元**。

#### 早期分词的基础思路：纯硬编码规则（最早期、最简单）

直接按字符/单词/标点做固定切分，完全写死在代码里，无任何统计或学习，适合简单场景。

**例如**：

**中文**：按单字切分（BERT-base-chinese 就是这种）
- **规则**：把句子按单个汉字拆分
- **示例**：「我爱学习编程」→「我、爱、学、习、编、程」
- **本质**：按字符编码（如 UTF-8）做遍历切分

#### 进阶思路：统计 + 规则的轻量算法（现代大模型主流）

如 GPT/BERT 的子词切分。

为了解决「单字/单词切分粒度太粗/太细」的问题，通过对海量语料做统计分析，得到一套「子词切分规则」，再按规则切分，核心是**「高频整词保留，低频词拆分为子词」**，兼顾语义性和复用性。

如此便能：

**示例**：「我爱学习编程」→「我、爱、学习、编程」

这便是海量语料统计分析的成果。

如此，我们便完成了这一步的目的，**把一个连续的自然语言拆成有语义、可复用的最小单元，叫做 Token**。

### Tokenizer 的第二步：给予 Token 数字 ID

在自然语言里，每个 Token 都不一样，哪怕表达的意思一样也会有不一样的外貌，例如：我、吾、我们、吾等，每个 Token 都应该有个独特的 ID 作为区分以免串味。

那怎么给呢？在中文的视角来看，就是搞个**新华字典电子版**，一个超大的词汇表，里面有尽可能多且独特的中文字符，并给予它们 ID。

我们先搞个缩小版的：

```python
vocab = ["[PAD]", "[CLS]", "[SEP]", "[UNK]", "苹果", "香蕉", "水泥", "我", "爱", "学习", "编程", "吃", "每天"]
```

这里 `vocab` 就是一个有序的列表，里面装满了 Token，然后我们把这个列表改造成字典给予 Token 们 ID：

```python
token2id = {token: idx for idx, token in enumerate(vocab)}

id2token = {idx: token for token, idx in token2id.items()}
```

我们再打印映射关系出来看看：
![映射关系](/my-learning-site/images/token与id的映射.png)

如此我们便得到了 Token 与 ID 的映射关系，也完成了我们这一步的目的，**为每个 Token 分配一个独特的数字 ID**。

上面是我们手动定义的，实际上最重要的其实就是 `vocab`。

例如在 `bert-base-chinese` 所使用的词汇表中，我们上面用到的例子的 ID 情况为：

![自动化映射关系](/my-learning-site/images/自动化.png)

因此我们无需再人工一个一个手动硬编码，可以直接让机器去学习一个词汇表，它会自动帮我们将词汇表词汇 Token 化，赋予 ID。

### 小结

如此我们便描绘出了 Tokenizer 的全貌了：**让机器去学习一个词汇表，自动将词汇表词汇 Token 化，并赋予 ID**。

## 关于 [PAD]、[CLS] 等这些奇怪标签 Token

这些带 `[]` 的特殊 Token（比如 `[CLS]`、`[SEP]`、`[PAD]`、`[UNK]`）并不是 Transformer **原始架构论文**里严格要求的「必备项」，但却是把 Transformer 落地到实际 NLP 任务（比如文本分类、问答、翻译）时的**工程标配**——没有它们，模型很难处理真实场景的文本数据。

我把这些特殊 Token 的「必备程度」和作用拆清楚，你就知道为什么它们几乎是必加的了：

> **背景**：Transformer 原论文（《Attention Is All You Need》）里做机器翻译时，其实只用了 `[PAD]`（补位）和 `<s>`/`</s>`（句子起止符），但后来 BERT/GPT 等模型把这些特殊 Token 标准化成 `[]` 形式（比如 `[CLS]` 替代 `<s>`），成了行业通用做法。

### 核心特殊 Token 的「必备性」拆解

#### 1. `[PAD]`（补位 Token）→ 真正的「刚需」（没有就跑不起来）
- **作用**：Transformer 是**并行计算**，要求一个批次（batch）里的所有文本序列长度必须完全一致。
  
  比如一个 batch 里有两句话：
  - 句子1：`我爱吃苹果`（5 个 Token）
  - 句子2：`我每天学习编程`（7 个 Token）
  
  必须用 `[PAD]` 把句子1补成 7 个 Token：`我 爱 吃 苹果 [PAD] [PAD]`，才能让模型批量处理。

- **没有的后果**：模型会因为输入序列长度不一致直接报错，根本无法训练/推理。

#### 2. `[UNK]`（未登录词 Token）→ 刚需（没有就报错）

- **作用**：词汇表不可能覆盖所有词（比如生僻词、新词、外文词，总而言之就是不在你的词汇表里），遇到不在词汇表里的词，必须用 `[UNK]` 替代，否则模型找不到对应的 ID，直接报错。
- **类比**：就像查字典时遇到不认识的字，用「□」替代，至少能继续读下去，而不是直接卡住。

#### 3. `[CLS]`（句子分类 Token）→ 分类任务「标配」（非所有任务必备）

- **作用**：Transformer 处理的是「序列数据」（比如一句话），但文本分类、情感分析等任务需要「整句话的语义向量」（比如判断「我爱吃苹果」是正面情绪）。
  
  `[CLS]` 放在句子开头，模型训练时会把整句话的语义编码到这个 Token 的向量里，后续分类层直接用这个向量做预测就行。

- **场景**：文本分类、情感分析必须有；机器翻译、文本生成可以不用（用 `<s>`/`</s>` 替代）。

#### 4. `[SEP]`（分隔 Token）→ 多句子任务「标配」（单句任务可省）

- **作用**：处理需要区分多句话的任务（比如问答：「问题：苹果好吃吗？答案：好吃」、文本匹配：「句子1：我爱吃苹果 句子2：我爱吃香蕉」），`[SEP]` 是「句子分隔符」，告诉模型「这里是第一句结束，第二句开始」。
- **场景**：问答、文本匹配、多句推理必须有；单句分类/生成可以不用。

#### 补充：`[MASK]`（掩码 Token）→ BERT 预训练专用

只在 BERT 预训练（掩码语言模型任务）时用，比如把「我爱吃苹果」改成「我爱吃[MASK]」，让模型预测 `[MASK]` 是啥，属于预训练阶段的特殊 Token，下游任务（比如分类）不用。

### 通俗类比：特殊 Token =「机器的标点符号」

你可以把这些 Token 理解成「给机器看的标点符号」：

- `[PAD]` = 作文里的「空格补位」，让所有句子行数一致
- `[UNK]` = 作文里的「□」，替代不认识的字
- `[CLS]` = 作文的「标题栏」，汇总整段话的核心意思
- `[SEP]` = 作文的「分段符」，区分不同段落

没有这些「标点」，人类读作文还能勉强猜，但机器完全看不懂，必须加这些「机器能识别的标点」才能正常工作。

### 总的来说

1. **真刚需（没有就跑不起来）**：`[PAD]`（补位）、`[UNK]`（未登录词）
2. **任务级标配（看场景）**：`[CLS]`（分类）、`[SEP]`（多句）、`[MASK]`（预训练）
3. 这些 Token 和普通词一样，需要加入词汇表并分配 ID，模型会像学习普通词的嵌入向量一样，学习它们的功能表征

当然了这些也是有基于统计和规则的算法来自动实现的，这里我们了解就好啦。

---

差不多讲完了，我们来做个总结吧。

## 总结
1. **嵌入的核心价值**：解决数字无语义的问题，实现「语义关联 → 数学关系」的映射
2. **Tokenizer 的本质**：自然语言到模型输入的前置转换器，核心是「切分 Token + ID 映射 + 格式标准化」
3. **Token 切分的原理**：纯规则/轻量统计算法，先于大模型存在，是大模型训练的前置基础
4. **vocab 的核心逻辑**：有序列表是源数据，`token2id`/`id2token` 是查询工具，实现 Token 和 ID 的唯一映射
5. **特殊 Token 的分类**：真刚需（`[PAD]`/`[UNK]`，没有就跑不起来）和任务级标配（`[CLS]`/`[SEP]`/`[MASK]`，看场景使用）